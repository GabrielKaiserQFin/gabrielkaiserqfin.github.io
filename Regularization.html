<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Curse of Dimensionality</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="Standarstyle.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 64px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h2 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h3 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h4 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h5 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h6 {
  padding-top: 69px;
  margin-top: -69px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Gabriel Kaiser</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Research.html">Research</a>
</li>
<li>
  <a href="teaching.html">Teaching</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="IntroductionPortfolioManagement.html">Introduction to Portfolio Management</a>
    </li>
    <li>
      <a href="Markowitz.html">Minimum Variance Portfolio</a>
    </li>
    <li>
      <a href="CAPM.html">CAPM</a>
    </li>
    <li>
      <a href="Regularization.html">Curse of Dimensionality</a>
    </li>
    <li>
      <a href="Invariance.html">Invariance</a>
    </li>
    <li>
      <a href="RBase.html">Fun with RBase</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="http://github.com/gabrielkaiserqfin">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/QuantGk">
    <span class="fa fa-twitter fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/gabriel-kaiser-a98a0083/">
    <span class="fa fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Curse of Dimensionality</h1>

</div>


<!-- \newcommand{\Var}{\operatorname{Var}} -->
<!-- \newcommand{\argmin}{\operatorname{\arg\!\min}} -->
<p>This is a short summary how to handle the curse of dimensionality in a linear model framework using R. The methods to overcome this trade-off are called <strong>Regularization</strong> or <strong>Penalization</strong>. I use <span class="math inline">\(R(.)\)</span> to denote a regularization function, however <span class="math inline">\(P(.)\)</span> is also often considered in the literature. These methods are also applicable to non-linear functions as for instance Poisson, Cox or logit model (logistic regression).</p>
<div id="bias-and-efficiency-trade-off" class="section level3">
<h3>Bias and efficiency trade-off:</h3>
<p>In general, regularization methods try to optimize the trade-off between model bias and efficiency. If we assume a linear model such as <span class="math inline">\(y =f(X) + \epsilon\)</span>, where the innovation term is standard Gaussian with variance <span class="math inline">\(\sigma^2\)</span>. Then under the assumption of linearity, full rank (non-singular matrix X), strict exogeneity, and homoscedasticity (uncorrelatedness) the <strong>Gauss-Markov Theorem</strong> suggests that the MSE estimator is a <strong>BLUE</strong>. What is more, X can be stochastic and if our assumption of normal distributed residuals hold, then our estimator has minimum variance across all unbiased estimators and is thus efficient. The decomposition of the MSE leads to three parts: <span class="math display">\[{\mathbb E} {\Big [}{\big (}y-{\hat {f}}(X){\big )}^{2}{\Big ]} = {\Big (}\operatorname {Bias} {\big [}{\hat {f}}(X){\big ]}{\Big )}^{2}+\operatorname {Var} {\big [}{\hat {f}}(X){\big ]}\]</span> with a bias given by <span class="math display">\[\text{Bias} {\big [}{\hat {f}}(X){\big ]}={\mathbb E} {\big [}{\hat {f}}(X){\big ]}-f(X)\]</span> and variance by <span class="math display">\[\text{Var} {\big [}{\hat {f}}(X){\big ]}={\mathbb E} [{\hat {f}}(X)^{2}]-{\mathbb E} [{\hat {f}}(X)]^{2}\]</span> However, since we are in a linear framework we can also write bias and variance in terms of the parameters itself, i.e. <span class="math display">\[\text{Bias}[{\hat {\theta }}]={\mathbb E}[\hat{\theta}]-\theta ={\mathbb E}[\hat{\theta }-\theta]\]</span> and <span class="math display">\[\text{Var}[{\hat {\theta }}]={\mathbb E}[\hat{\theta^2}]-{\mathbb E}[\hat{\theta}]^2\]</span> With increasing dimensionality of <span class="math inline">\(X\)</span> grows the dimension of <span class="math inline">\(\theta\)</span> and hence the variance given a very small bias. This is called the the <strong>curse of dimensionality</strong>. So large variance or inefficiency leads to over-fitting whereas a large bias leads to parameter estimates that under fit the true parameters.</p>
<p>We continue with the most common linear model, the MSE is the first part of our objective function but could be replaced by any other linear function <span class="math inline">\(f(.)\)</span>: <span class="math display">\[\hat{\beta}= \min_{\hat{\beta}}  \frac{1}{2n} \Vert y - X\beta \Vert^2+\lambda R(\beta) \]</span> <span class="math inline">\(\lambda\)</span> is often called tuning parameter and actually determines the weight at which the linear model is penalized.</p>
</div>
<div id="ridge-regression" class="section level3">
<h3>Ridge Regression:</h3>
<p>The ridge regression applies the Tikhonov regularization, which is the <span class="math inline">\(l_2\)</span> norm, i.e. <span class="math display">\[R(w) = \frac{1}{2n}{\Vert \beta\Vert}_2^2=\frac{1}{2n}\sum_{i=1}^{n}\beta^2_i\]</span> It controls the complexity of the model and is differentiable and thus easily optimizable.</p>
</div>
<div id="lasso" class="section level3">
<h3>Lasso:</h3>
<p>The Lasso algorithm utilizes the <span class="math inline">\(l_1\)</span> norm instead, i.e. <span class="math display">\[R(w) = \frac{1}{n}{\Vert \beta\Vert}_1=\frac{1}{n}\sum_{i=1}^{n}|\beta_i|\]</span> It also controls the comlpexity of the model but is non-differentiable around zero and thus harder to optimze. It pushes the entries towards zero and thus enforces sparsity.</p>
</div>
<div id="group-lasso" class="section level3">
<h3>Group Lasso:</h3>
<p>Another regularizer is the <span class="math inline">\(l_{2,1}\)</span> norm, i.e. <span class="math display">\[R(w) = \frac{1}{n}{\Vert \beta\Vert}_{2,1}=\frac{1}{n}\sum_{g=1}^{n_g} \sqrt{ \sum_{f=1}^{n_f}\beta^2_{g,f}}\]</span> It also controls the comlpexity of the model but the <span class="math inline">\(l_{2,1}\)</span> norm is not squared and hence non-differentiable around zero and thus harder to optimze. It pushes the entries towards zero and thus enforces sparsity at the group level.</p>
</div>
<div id="fused-lasso" class="section level3">
<h3>Fused Lasso:</h3>
<p>Transformed norms are similar as above just that regularization is applied over a linear transformation, i.e. <span class="math inline">\(w=T\beta\)</span>, which can be applied to all three regularization methods. The most common example of transformed norms is the <strong>Total Variation</strong>, which is a special family of regularizers that penalize the differences between adjacent (neighboring) entries and hence assumes spatial location. In this case <span class="math inline">\(T\)</span> is a differentiating matrix, such that <span class="math inline">\(T_{i,i}=-1\)</span>, <span class="math inline">\(T_{i,i+1}=1\)</span> and zero otherwise. <span class="math display">\[R(\beta) = \frac{1}{n}{\Vert T\beta\Vert}_{1}\]</span> Other examples for transformed norms are <strong>Graph-Based Total Variation</strong> or <strong>Trend Filtering</strong>.</p>
</div>
<div id="elastic-network" class="section level3">
<h3>Elastic-Network:</h3>
<p>Several combinations of the above mentioned methods exist as well. The most common is the <strong>Elastic-Network</strong> model, which is just the combination between Ridge and Lasso, i.e. <span class="math display">\[R(w) = \frac{1}{n}{\Vert \beta\Vert}_1 +\frac{1}{2n}{\Vert \beta\Vert}_2^2=\frac{1}{n}\sum_{i=1}^{n}|\beta_i|+\frac{1}{2n}\sum_{i=1}^{n}\beta^2_i\]</span> Typically, these functions are weighted independently from each other, leading to two tuning parameter <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>.</p>
</div>
<div id="adaptive-lasso" class="section level3">
<h3>Adaptive Lasso:</h3>
<p><a href="http://pages.cs.wisc.edu/~shao/stat992/zou2006.pdf">‘<span class="citation">Zou (2006)</span>’</a> state that the standard lasso model does not have the <strong>Oracel</strong> properties. The oracle procedure ensures an optimal estimation rate and thus is more efficient. Moreover, it provides unbiased estimates. The procedure includes an adaptive weight vector <span class="math inline">\(w_i = |\hat{\beta_i}|^{-\gamma}\)</span>, where the parameter vector <span class="math inline">\(\beta\)</span> is typically estimated by a Ridge regression. Consequently, adaptive lasso penalizes those coefficients with lower initial estimates more. The adjustment parameter <span class="math inline">\(\gamma\)</span> is non-negaive and typically in the set {0.5, 1, 2}. <span class="math display">\[R(w) = \frac{1}{n}{\Vert w&#39; \beta\Vert}_1=\frac{1}{n}\sum_{i=1}^{n}w_i|\beta_i|\]</span></p>
</div>
<div id="code" class="section level3">
<h3>Code:</h3>
<p>In R, several packages allow us to estimate a model applying various regularization methods. For instance, the package <strong>glmnet</strong> implements the lasso or ridge penalty for generalized linear models via penalized maximum likelihood. <span class="math display">\[\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}}\frac{1}{2N} \sum_{i=1}^N (y_i -\beta_0-x_i^T \beta)^2+\lambda \left[ (1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1\right]\]</span> In order to obtain the Ridge algorithm we just need to set all the weight on the <span class="math inline">\(l2\)</span> norm, i.e. <span class="math inline">\(\alpha=0\)</span>. The cross validation glmnet function already performs n-fold cross validation to choose the best <span class="math inline">\(\lambda\)</span>.</p>
<pre class="r"><code>require(glmnet, quietly = TRUE)
# load some data
x &lt;- as.matrix(trees[, -1]) # independent variables
y &lt;- as.double(as.matrix(trees[, 1])) # dependent variables
## Ridge Regression to create the Adaptive Weights Vector: alpha = 0
gamma &lt;- 1
ridge &lt;-
  cv.glmnet(x,
    y,
    type.measure = &quot;mse&quot;,
    family = &quot;gaussian&quot;,
    standardize = TRUE,
    alpha=0
  )
w &lt;- 1 / abs(matrix(coef(ridge, s = ridge$lambda.min)[, 1][2:(ncol(x) + 1)]))^gamma
w[is.infinite(w)] &lt;- 10^12 # Replacing Infinite values
## Adaptive Lasso: alpha = 1
fit &lt;-
  cv.glmnet(
    x,
    y,
    alpha = 1,
    nfold = 10,
    standardize = TRUE,
    type.measure = &quot;mse&quot;,
    family = &quot;gaussian&quot;,
    penalty.factor = w
  )
plot(fit)</code></pre>
<p><img src="Regularization_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>plot(fit$glmnet.fit, xvar = &quot;lambda&quot;, label = TRUE)
abline(v = log(fit$lambda.min))
abline(v = log(fit$lambda.1se))</code></pre>
<p><img src="Regularization_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code>coef(fit, s = fit$lambda.1se)</code></pre>
<pre><code>## 3 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                     1
## (Intercept) 8.5444511
## Height      .        
## Volume      0.1559094</code></pre>
<pre class="r"><code>coef &lt;- coef(fit, s = &quot;lambda.1se&quot;)
selected_attributes &lt;- (coef@i[-1] + 1)
# lasso.pred &lt;- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)

# plot(fit$glmnet,&quot;lambda&quot;,ylim=c(-2,2))</code></pre>
<div id="refs" class="references">
<div id="ref-zou2006adaptive">
<p>Zou, Hui. 2006. “The Adaptive Lasso and Its Oracle Properties.” <em>Journal of the American Statistical Association</em> 101 (476): 1418–29.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
